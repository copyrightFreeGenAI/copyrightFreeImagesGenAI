{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l6kL2-U8MZkE"
      ],
      "authorship_tag": "ABX9TyM0kvssNl0cpes6TfrlcJbh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/copyrightFreeGenAI/copyrightFreeImagesGenAI/blob/main/3.%20Fine-Tuning/Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQwi7kVdyQs-"
      },
      "outputs": [],
      "source": [
        "# Set Paths\n",
        "annTrainInstanceFiles = \"/content/gdrive/MyDrive/annotations2014/instances_val2014.json\" # Path to fine-tuning instance json\n",
        "annTrainCaptionFiles = \"/content/gdrive/MyDrive/annotations2014/captions_val2014.json\" # Path to fine-tuning train caption json\n",
        "train_images_directory = \"/content/gdrive/MyDrive/DATA/COCO/Fine-Tuning/Images\" # Path to fine-tuning directory\n",
        "\n",
        "DREAM_WORKSPACE = '/content/gdrive/MyDrive/Fast-Dreambooth' # Path to store DreamBooth sessions\n",
        "LORA_WORKSPACE = '/content/gdrive/MyDrive/LoRA' # Path to store LoRA sessions\n",
        "TI_WORKSPACE = '/content/gdrive/MyDrive/Textual-Inversion' # Path to store Textual Inversion sessions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "vVLZif2mL3fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools.coco import COCO\n",
        "coco=COCO(annTrainInstanceFiles)\n",
        "coco_caps=COCO(annTrainCaptionFiles)\n",
        "cats = coco.loadCats(coco.getCatIds())"
      ],
      "metadata": {
        "id": "jR49dkH3OKMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caption and Rename MS COCO Fine-tuning Dataset"
      ],
      "metadata": {
        "id": "l6kL2-U8MZkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "wVo-nMBfOOnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in cats:\n",
        "  # Construct the directory path for the current category\n",
        "  directory = os.path.join(train_images_directory, cat['name'])\n",
        "  # Skip specified categories\n",
        "  if cat[\"name\"] in [\"handbag\", \"sports ball\", \"toaster\", \"hair drier\"]:\n",
        "    continue\n",
        "  # Get category and image IDs\n",
        "  catIds = coco.getCatIds(catNms=[cat['name']])\n",
        "  imgIds = coco.getImgIds(catIds=catIds)\n",
        "  for filename in os.listdir(directory):\n",
        "      # Check if the corresponding .txt file already exists\n",
        "      base_name, extension = os.path.splitext(filename)\n",
        "      txt_dir = os.path.join(directory, f\"{base_name}.txt\")\n",
        "      if os.path.exists(txt_dir):\n",
        "          continue\n",
        "      # Load image and annotations\n",
        "      img = coco.loadImgs(imgIds[int(base_name)])[0]\n",
        "      annIds = coco_caps.getAnnIds(imgIds=img['id'])\n",
        "      anns = coco_caps.loadAnns(annIds)\n",
        "      # Create the prompt by joining all captions\n",
        "      captions_list = [item['caption'].lower() for item in anns]\n",
        "      prompt = \"qwer, \" + ' '.join(captions_list)\n",
        "      prompt = prompt.replace(cat['name'], 'qwer')\n",
        "      # Write the prompt to the .txt file\n",
        "      with open(txt_dir, \"w\") as f:\n",
        "          f.write(prompt)"
      ],
      "metadata": {
        "id": "IhUvJJ02MjT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in cats:\n",
        "  # Construct the directory path for the current category\n",
        "  directory = os.path.join(train_images_directory, cat['name'])\n",
        "  # Skip specified categories\n",
        "  if cat[\"name\"] in [\"handbag\", \"sports ball\", \"toaster\", \"hair drier\"]:\n",
        "    continue\n",
        "  # Iterate over files in the directory\n",
        "  for count, filename in enumerate(os.listdir(directory)):\n",
        "    # Rename the file with 'qwer' prefix and original extension\n",
        "    new_filename = f\"qwer ({os.path.splitext(filename)[0]}){os.path.splitext(filename)[1]}\"\n",
        "    os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))"
      ],
      "metadata": {
        "id": "Cv7uJNJjMutu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DreamBooth"
      ],
      "metadata": {
        "id": "Zr88VbJdNrm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Dependencies\n",
        "\n",
        "from IPython.utils import capture\n",
        "import time\n",
        "import os\n",
        "\n",
        "print('\u001b[1;32mInstalling dependencies...')\n",
        "with capture.capture_output() as cap:\n",
        "    os.chdir('/content')\n",
        "    !pip uninstall diffusers wandb -qq -y\n",
        "    !pip install -qq --no-deps accelerate==0.12.0\n",
        "    !wget -q -i https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dependencies/dbdeps.txt\n",
        "    !dpkg -i *.deb\n",
        "    !tar -C / --zstd -xf gcolabdeps.tar.zst\n",
        "    !rm *.deb | rm *.zst | rm *.txt\n",
        "    !git clone -q --depth 1 --branch main https://github.com/TheLastBen/diffusers\n",
        "    !pip install gradio==3.16.2 wandb==0.15.12 pydantic==1.10.2 -qq\n",
        "\n",
        "    if not os.path.exists('gdrive/MyDrive/sd/libtcmalloc/libtcmalloc_minimal.so.4'):\n",
        "        %env CXXFLAGS=-std=c++14\n",
        "        !wget -q https://github.com/gperftools/gperftools/releases/download/gperftools-2.5/gperftools-2.5.tar.gz && tar zxf gperftools-2.5.tar.gz && mv gperftools-2.5 gperftools\n",
        "        !wget -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/AUTOMATIC1111_files/Patch\n",
        "        %cd /content/gperftools\n",
        "        !patch -p1 < /content/Patch\n",
        "        !./configure --enable-minimal --enable-libunwind --enable-frame-pointers --enable-dynamic-sized-delete-support --enable-sized-delete --enable-emergency-malloc; make -j4\n",
        "        !mkdir -p /content/gdrive/MyDrive/sd/libtcmalloc && cp .libs/libtcmalloc*.so* /content/gdrive/MyDrive/sd/libtcmalloc\n",
        "        %env LD_PRELOAD=/content/gdrive/MyDrive/sd/libtcmalloc/libtcmalloc_minimal.so.4\n",
        "        %cd /content\n",
        "        !rm *.tar.gz Patch && rm -r /content/gperftools\n",
        "    else:\n",
        "        %env LD_PRELOAD=/content/gdrive/MyDrive/sd/libtcmalloc/libtcmalloc_minimal.so.4\n",
        "\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "    os.environ['PYTHONWARNINGS'] = 'ignore'\n",
        "    !sed -i 's@raise AttributeError(f\"module {module!r} has no attribute {name!r}\")@@g' /usr/local/lib/python3.11/dist-packages/jax/_src/deprecations.py\n",
        "\n",
        "print('\u001b[1;32mDone, proceed')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eovL-e3wNuHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from IPython.utils import capture\n",
        "from IPython.display import clear_output\n",
        "import wget\n",
        "from subprocess import check_output\n",
        "import urllib.request\n",
        "import requests\n",
        "import base64\n",
        "from gdown.download import get_url_from_gdrive_confirmation\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "from urllib.request import urlopen, Request\n",
        "import re\n",
        "\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  os.chdir('/content')\n",
        "\n",
        "Path_to_HuggingFace= \"Mitsua/mitsua-diffusion-one\"\n",
        "\n",
        "\n",
        "if os.path.exists('/content/gdrive/MyDrive/Fast-Dreambooth/token.txt'):\n",
        "  with open(\"/content/gdrive/MyDrive/Fast-Dreambooth/token.txt\") as f:\n",
        "     token = f.read()\n",
        "  authe=f'https://USER:{token}@'\n",
        "else:\n",
        "  authe=\"https://\"\n",
        "\n",
        "\n",
        "if Path_to_HuggingFace != \"\":\n",
        "  if authe==\"https://\":\n",
        "    textenc= f\"{authe}huggingface.co/{Path_to_HuggingFace}/resolve/main/text_encoder/pytorch_model.bin\"\n",
        "    txtenc_size=urllib.request.urlopen(textenc).info().get('Content-Length', None)\n",
        "  else:\n",
        "    textenc= f\"https://huggingface.co/{Path_to_HuggingFace}/resolve/main/text_encoder/pytorch_model.bin\"\n",
        "    req=urllib.request.Request(textenc)\n",
        "    req.add_header('Authorization', f'Bearer {token}')\n",
        "    txtenc_size=urllib.request.urlopen(req).info().get('Content-Length', None)\n",
        "  if int(txtenc_size)> 670000000 :\n",
        "    if os.path.exists('/content/stable-diffusion-custom'):\n",
        "      !rm -r /content/stable-diffusion-custom\n",
        "    clear_output()\n",
        "    os.chdir('/content')\n",
        "    clear_output()\n",
        "    print(\"\u001b[1;32mV2\")\n",
        "    !mkdir /content/stable-diffusion-custom\n",
        "    os.chdir('/content/stable-diffusion-custom')\n",
        "    !git config --global init.defaultBranch main\n",
        "    !git init\n",
        "    !git lfs install --system --skip-repo\n",
        "    !git remote add -f origin  \"{authe}huggingface.co/{Path_to_HuggingFace}\"\n",
        "    !git config core.sparsecheckout true\n",
        "    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nfeature_extractor\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n",
        "    !git pull origin main\n",
        "    if os.path.exists('/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
        "      !rm -r /content/stable-diffusion-custom/.git\n",
        "      os.chdir('/content')\n",
        "      MODEL_NAME=\"/content/stable-diffusion-custom\"\n",
        "      clear_output()\n",
        "      print('\u001b[1;32mDONE !')\n",
        "    else:\n",
        "      while not os.path.exists('/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
        "            print('\u001b[1;31mCheck the link you provided')\n",
        "            time.sleep(5)\n",
        "  else:\n",
        "    if os.path.exists('/content/stable-diffusion-custom'):\n",
        "      !rm -r /content/stable-diffusion-custom\n",
        "    clear_output()\n",
        "    os.chdir('/content')\n",
        "    clear_output()\n",
        "    print(\"\u001b[1;32mV1\")\n",
        "    !mkdir /content/stable-diffusion-custom\n",
        "    os.chdir('/content/stable-diffusion-custom')\n",
        "    !git init\n",
        "    !git lfs install --system --skip-repo\n",
        "    !git remote add -f origin  \"{authe}huggingface.co/{Path_to_HuggingFace}\"\n",
        "    !git config core.sparsecheckout true\n",
        "    !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\\n!*.safetensors\" > .git/info/sparse-checkout\n",
        "    !git pull origin main\n",
        "    if os.path.exists('/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
        "      !rm -r /content/stable-diffusion-custom/.git\n",
        "      !rm model_index.json\n",
        "      time.sleep(1)\n",
        "      wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
        "      os.chdir('/content')\n",
        "      MODEL_NAME=\"/content/stable-diffusion-custom\"\n",
        "      clear_output()\n",
        "      print('\u001b[1;32mDONE !')\n",
        "    else:\n",
        "      while not os.path.exists('/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
        "            print('\u001b[1;31mCheck the link you provided')\n",
        "            time.sleep(5)"
      ],
      "metadata": {
        "id": "hdnGSdDQN26h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dump_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, precision, Training_Steps):\n",
        "    # Construct the bash command using f-strings\n",
        "    bash_command = f\"\"\"\n",
        "    accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py --external_captions \\\n",
        "        --image_captions_filename \\\n",
        "        --train_text_encoder \\\n",
        "        --dump_only_text_encoder \\\n",
        "        --pretrained_model_name_or_path=\"{MODELT_NAME}\" \\\n",
        "        --instance_data_dir=\"{INSTANCE_DIR}\" \\\n",
        "        --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "        --captions_dir=\"{CAPTIONS_DIR}\" \\\n",
        "        --instance_prompt=\"{PT}\" \\\n",
        "        --resolution={TexRes} \\\n",
        "        --mixed_precision=\"{precision}\" \\\n",
        "        --train_batch_size=1 \\\n",
        "        --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
        "        --use_8bit_adam \\\n",
        "        --learning_rate={txlr} \\\n",
        "        --lr_scheduler=\"linear\" \\\n",
        "        --lr_warmup_steps=0 \\\n",
        "        --max_train_steps={Training_Steps}\n",
        "    \"\"\"\n",
        "\n",
        "    !{bash_command}\n",
        "\n",
        "\n",
        "def train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Res, precision, Training_Steps):\n",
        "    from IPython.display import clear_output\n",
        "    clear_output()\n",
        "    print('\\033[1;33mTraining the UNet...\\033[0m')\n",
        "\n",
        "    # Construct the bash command using f-string for clarity\n",
        "    bash_command = f\"\"\"\n",
        "    accelerate launch /content/diffusers/examples/dreambooth/train_dreambooth.py --external_captions \\\n",
        "        --image_captions_filename \\\n",
        "        --train_only_unet \\\n",
        "        --save_starting_step={stpsv} \\\n",
        "        --save_n_steps={stp} \\\n",
        "        --Session_dir=\"{SESSION_DIR}\" \\\n",
        "        --pretrained_model_name_or_path=\"{MODELT_NAME}\" \\\n",
        "        --instance_data_dir=\"{INSTANCE_DIR}\" \\\n",
        "        --output_dir=\"{OUTPUT_DIR}\" \\\n",
        "        --captions_dir=\"{CAPTIONS_DIR}\" \\\n",
        "        --instance_prompt=\"{PT}\" \\\n",
        "        --resolution={Res} \\\n",
        "        --mixed_precision=\"{precision}\" \\\n",
        "        --train_batch_size=1 \\\n",
        "        --gradient_accumulation_steps=1 {GCUNET} \\\n",
        "        --use_8bit_adam \\\n",
        "        --learning_rate={untlr} \\\n",
        "        --lr_scheduler=\"linear\" \\\n",
        "        --lr_warmup_steps=0 \\\n",
        "        --max_train_steps={Training_Steps}\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the command\n",
        "    !{bash_command}"
      ],
      "metadata": {
        "id": "OsY_8He_OahL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install bitsandbytes --no-cache-dir"
      ],
      "metadata": {
        "id": "YV_x30d4OsIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replace /content/diffusers/examples/dreambooth/train_dreambooth.py with provided train_dreambooth.py file**"
      ],
      "metadata": {
        "id": "Sksgwdx6VQFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "import time\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import ipywidgets as widgets\n",
        "from io import BytesIO\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "from IPython.utils import capture\n",
        "from os import listdir\n",
        "from os.path import isfile\n",
        "from subprocess import check_output\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "from google.colab import runtime\n",
        "from subprocess import getoutput\n",
        "import time\n",
        "import random\n",
        "\n",
        "for cat in cats:\n",
        "    print(cat['name'])\n",
        "    if cat['name'] in ['handbag', 'sports ball', 'toaster', 'hair drier']:\n",
        "      continue\n",
        "    if cat['name'].replace(' ', '_') in seen:\n",
        "        continue\n",
        "    Session_Name = cat['name'].replace(' ', '_')\n",
        "    INSTANCE_NAME=Session_Name\n",
        "    OUTPUT_DIR=\"/content/models/\"+Session_Name\n",
        "    SESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\n",
        "    INSTANCE_DIR=SESSION_DIR+'/instance_images'\n",
        "    CAPTIONS_DIR=SESSION_DIR+'/captions'\n",
        "    MDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n",
        "    %mkdir -p \"$INSTANCE_DIR\"\n",
        "    IMAGES_FOLDER_OPTIONAL=train_images_directory + Session_Name\n",
        "    Remove_existing_instance_images= False\n",
        "    if Remove_existing_instance_images:\n",
        "      if os.path.exists(str(INSTANCE_DIR)):\n",
        "        !rm -r \"$INSTANCE_DIR\"\n",
        "      if os.path.exists(str(CAPTIONS_DIR)):\n",
        "        !rm -r \"$CAPTIONS_DIR\"\n",
        "    if not os.path.exists(str(INSTANCE_DIR)):\n",
        "      %mkdir -p \"$INSTANCE_DIR\"\n",
        "    if not os.path.exists(str(CAPTIONS_DIR)):\n",
        "      %mkdir -p \"$CAPTIONS_DIR\"\n",
        "    if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
        "      %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n",
        "    with capture.capture_output() as cap:\n",
        "      !mv $IMAGES_FOLDER_OPTIONAL/*.txt $CAPTIONS_DIR\n",
        "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
        "      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
        "    with capture.capture_output() as cap:\n",
        "      %cd \"$INSTANCE_DIR\"\n",
        "      !find . -name \"* *\" -type f | rename 's/ /-/g'\n",
        "      %cd \"$CAPTIONS_DIR\"\n",
        "      !find . -name \"* *\" -type f | rename 's/ /-/g'\n",
        "\n",
        "      %cd $SESSION_DIR\n",
        "      !rm instance_images.zip captions.zip\n",
        "      !zip -r instance_images instance_images\n",
        "      !zip -r captions captions\n",
        "      %cd /content\n",
        "\n",
        "    # TRAINING\n",
        "    if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
        "      %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n",
        "    if os.path.exists(CAPTIONS_DIR+\"/.ipynb_checkpoints\"):\n",
        "      %rm -r $CAPTIONS_DIR\"/.ipynb_checkpoints\"\n",
        "    MODELT_NAME=MODEL_NAME\n",
        "    UNet_Training_Steps=2000\n",
        "    UNet_Learning_Rate = 2e-6\n",
        "    untlr=UNet_Learning_Rate\n",
        "    Text_Encoder_Training_Steps=200\n",
        "    Text_Encoder_Learning_Rate = 1e-6\n",
        "    txlr=Text_Encoder_Learning_Rate\n",
        "    Resolution = \"512\"\n",
        "    Res=int(Resolution)\n",
        "    precision=\"no\"\n",
        "    V2=False\n",
        "    if os.path.getsize(MODELT_NAME+\"/text_encoder/pytorch_model.bin\") > 670901463:\n",
        "      V2=True\n",
        "    s = getoutput('nvidia-smi')\n",
        "    GCUNET=\"--gradient_checkpointing\"\n",
        "    TexRes=Res\n",
        "    if Res<=768:\n",
        "      GCUNET=\"\"\n",
        "    if V2:\n",
        "      if Res>704:\n",
        "        GCUNET=\"--gradient_checkpointing\"\n",
        "      if Res>576:\n",
        "        TexRes=576\n",
        "    if 'A100' in s :\n",
        "      GCUNET=\"\"\n",
        "      TexRes=Res\n",
        "    stptxt=Text_Encoder_Training_Steps\n",
        "    Save_Checkpoint_Every=500\n",
        "    stp=0\n",
        "    Start_saving_from_the_step=3000\n",
        "    stpsv=Start_saving_from_the_step\n",
        "    stp=Save_Checkpoint_Every\n",
        "\n",
        "    print('\u001b[1;33mTraining the text encoder...\u001b[0m')\n",
        "    if os.path.exists(OUTPUT_DIR+'/'+'text_encoder_trained'):\n",
        "      %rm -r $OUTPUT_DIR\"/text_encoder_trained\"\n",
        "    PT=\"\"\n",
        "    dump_only_textenc(MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, precision, Training_Steps=stptxt)\n",
        "\n",
        "    if UNet_Training_Steps!=0:\n",
        "      train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Res, precision, Training_Steps=UNet_Training_Steps)\n",
        "    if os.path.exists('/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n",
        "      prc=\"--fp16\" if precision==\"fp16\" else \"\"\n",
        "      !python /content/diffusers/scripts/convertosdv2.py $prc $OUTPUT_DIR $SESSION_DIR/$Session_Name\".ckpt\"\n",
        "      time.sleep(120)\n",
        "      if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n",
        "        print(\"\u001b[1;32mDONE, the CKPT model is in your Gdrive in the sessions folder\")"
      ],
      "metadata": {
        "id": "vkVXam0EOufW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA"
      ],
      "metadata": {
        "id": "UNLbziNmRuVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Workspace"
      ],
      "metadata": {
        "id": "y5N7ZhAdTvQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "Khap2dD0T-jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in cats:\n",
        "  print(cat['name'])\n",
        "  category = cat['name'].replace(\" \", \"_\")\n",
        "  target_dir_cat = os.path.join(LORA_WORKSPACE, category)\n",
        "  source_dir_cat = os.path.join(train_images_directory, category)\n",
        "  os.makedirs(target_dir_cat, exist_ok=True)\n",
        "  dataset_dir = os.path.join(target_dir_cat, \"dataset\")\n",
        "  os.makedirs(dataset_dir, exist_ok=True)\n",
        "  for filename in os.listdir(source_dir_cat):\n",
        "    source_file = os.path.join(source_dir_cat, filename)\n",
        "    target_file = os.path.join(dataset_dir, filename)\n",
        "    shutil.copy(source_file, target_file)"
      ],
      "metadata": {
        "id": "qRyVuHKPRwhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train LoRA"
      ],
      "metadata": {
        "id": "I00ZwW37UFFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate==0.26.0\n",
        "!pip install huggingface-hub==0.25.2\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "kXH2fTruT5d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "model_url = \"Mitsua/mitsua-diffusion-one\"\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "# These may be set by other cells, some are legacy\n",
        "if \"custom_dataset\" not in globals():\n",
        "  custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "  override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "  override_config_file = None\n",
        "if \"optimizer\" not in globals():\n",
        "  optimizer = \"AdamW8bit\"\n",
        "if \"optimizer_args\" not in globals():\n",
        "  optimizer_args = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "  continue_from_lora = \"\"\n",
        "if \"weighted_captions\" not in globals():\n",
        "  weighted_captions = False\n",
        "if \"adjust_tags\" not in globals():\n",
        "  adjust_tags = False\n",
        "if \"keep_tokens_weight\" not in globals():\n",
        "  keep_tokens_weight = 1.0\n",
        "\n",
        "COLAB = True # low ram\n",
        "XFORMERS = True\n",
        "SOURCE = \"https://github.com/uYouUs/sd-scripts\"\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True"
      ],
      "metadata": {
        "id": "8ssyg8p2UHKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\"\n",
        "custom_model_is_based_on_sd2 = False\n",
        "resolution = 512\n",
        "flip_aug = False\n",
        "caption_extension = \".txt\"\n",
        "shuffle_tags = False\n",
        "shuffle_caption = shuffle_tags\n",
        "activation_tags = \"1\"\n",
        "keep_tokens = int(activation_tags)\n",
        "num_repeats = 10\n",
        "preferred_unit = \"Epochs\"\n",
        "how_many = 20\n",
        "max_train_epochs = how_many if preferred_unit == \"Epochs\" else None\n",
        "max_train_steps = how_many if preferred_unit == \"Steps\" else None\n",
        "save_every_n_epochs = 10\n",
        "keep_only_last_n_epochs = 10\n",
        "if not save_every_n_epochs:\n",
        "  save_every_n_epochs = max_train_epochs\n",
        "if not keep_only_last_n_epochs:\n",
        "  keep_only_last_n_epochs = max_train_epochs\n",
        "train_batch_size = 2\n",
        "unet_lr = 5e-4\n",
        "text_encoder_lr = 1e-4\n",
        "lr_scheduler = \"cosine_with_restarts\"\n",
        "lr_scheduler_number = 3\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "lr_warmup_ratio = 0.05\n",
        "lr_warmup_steps = 0\n",
        "min_snr_gamma = True\n",
        "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
        "lora_type = \"LoRA\"\n",
        "network_dim = 64\n",
        "network_alpha = 32\n",
        "conv_dim = 8\n",
        "conv_alpha = 4\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "  network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]"
      ],
      "metadata": {
        "id": "QGDQM3zcUJXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if optimizer.lower() == \"prodigy\" or \"dadapt\" in optimizer.lower():\n",
        "\n",
        "  if not optimizer_args:\n",
        "    optimizer_args = [\"decouple=True\",\"weight_decay=0.01\",\"betas=[0.9,0.999]\"]\n",
        "    if optimizer == \"Prodigy\":\n",
        "      optimizer_args.extend([\"d_coef=2\",\"use_bias_correction=True\"])\n",
        "      if lr_warmup_ratio > 0:\n",
        "        optimizer_args.append(\"safeguard_warmup=True\")\n",
        "      else:\n",
        "        optimizer_args.append(\"safeguard_warmup=False\")\n",
        "\n",
        "root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")"
      ],
      "metadata": {
        "id": "DPScvG0wURA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def install_dependencies():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_network_wrapper.py -q -O train_network_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install -U torch==2.4 xformers triton torchvision==0.19 --index-url https://download.pytorch.org/whl/cu121\n",
        "  !pip install accelerate==0.26.0 transformers diffusers[torch]==0.25.0 ftfy==6.1.1 \\\n",
        "    opencv-python==4.8.1.78 einops==0.7.0 pytorch-lightning==1.9.0 bitsandbytes==0.43.0 \\\n",
        "    prodigyopt==1.0 lion-pytorch==0.0.6 tensorboard safetensors==0.4.2 altair==4.2.2 \\\n",
        "    easygui==0.98.3 toml==0.10.2 voluptuous huggingface-hub==0.25.2 imagesize==1.4.1 rich==13.7.1\n",
        "  !pip install -e .\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if COLAB:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, keep_tokens_weight, weighted_captions, adjust_tags\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "  print(\"\\n💿 Checking dataset...\")\n",
        "  if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"💥 Error: Please choose a valid project name.\")\n",
        "    return\n",
        "\n",
        "  if custom_dataset:\n",
        "    try:\n",
        "      datconf = toml.loads(custom_dataset)\n",
        "      datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "    except:\n",
        "      print(f\"💥 Error: Your custom dataset is invalid or contains an error! Please check the original template.\")\n",
        "      return\n",
        "    reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "    datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "    folders = datasets_dict.keys()\n",
        "    files = [f for folder in folders for f in os.listdir(folder)]\n",
        "    images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "  else:\n",
        "    reg = []\n",
        "    folders = [images_folder]\n",
        "    files = os.listdir(images_folder)\n",
        "    images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"💥 Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"💥 Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "  for f in files:\n",
        "    if not f.lower().endswith((\".txt\", \".npz\")) and not f.lower().endswith(supported_types):\n",
        "      print(f\"💥 Error: Invalid file in dataset: \\\"{f}\\\". Aborting.\")\n",
        "      return\n",
        "\n",
        "  if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "    caption_extension = \"\"\n",
        "  if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "    print(f\"💥 Error: Invalid path to existing Lora. Example: /content/drive/MyDrive/Loras/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = max_train_steps or int(max_train_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"📁\"+folder.replace(\"/content/drive/\", \"\") + (\" (Regularization)\" if folder in reg else \"\"))\n",
        "    print(f\"📈 Found {img} images with {rep} repeats, equaling {img*rep} steps.\")\n",
        "  print(f\"📉 Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n",
        "  if max_train_epochs:\n",
        "    print(f\"🔮 There will be {max_train_epochs} epochs, for around {total_steps} total training steps.\")\n",
        "  else:\n",
        "    print(f\"🔮 There will be {total_steps} steps, divided into {estimated_epochs} epochs and then some.\")\n",
        "\n",
        "  if total_steps > 10000:\n",
        "    print(\"💥 Error: Your total steps are too high. You probably made a mistake. Aborting...\")\n",
        "    return\n",
        "\n",
        "  if adjust_tags:\n",
        "    print(f\"\\n📎 Weighted tags: {'ON' if weighted_captions else 'OFF'}\")\n",
        "    if weighted_captions:\n",
        "      print(f\"📎 Will use {keep_tokens_weight} weight on {keep_tokens} activation tag(s)\")\n",
        "    print(\"📎 Adjusting tags...\")\n",
        "    adjust_weighted_tags(folders, keep_tokens, keep_tokens_weight, weighted_captions)\n",
        "\n",
        "  return True\n",
        "\n",
        "def adjust_weighted_tags(folders, keep_tokens: int, keep_tokens_weight: float, weighted_captions: bool):\n",
        "  weighted_tag = re.compile(r\"\\((.+?):[.\\d]+\\)(,|$)\")\n",
        "  for folder in folders:\n",
        "    for txt in [f for f in os.listdir(folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(folder, txt), 'r') as f:\n",
        "        content = f.read()\n",
        "      # reset previous changes\n",
        "      content = content.replace('\\\\', '')\n",
        "      content = weighted_tag.sub(r'\\1\\2', content)\n",
        "      if weighted_captions:\n",
        "        # re-apply changes\n",
        "        content = content.replace(r'(', r'\\(').replace(r')', r'\\)').replace(r':', r'\\:')\n",
        "        if keep_tokens_weight > 1:\n",
        "          tags = [s.strip() for s in content.split(\",\")]\n",
        "          for i in range(min(keep_tokens, len(tags))):\n",
        "            tags[i] = f'({tags[i]}:{keep_tokens_weight})'\n",
        "          content = \", \".join(tags)\n",
        "      with open(os.path.join(folder, txt), 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  if override_config_file:\n",
        "    config_file = override_config_file\n",
        "    print(f\"\\n⭕ Using custom config file {config_file}\")\n",
        "  else:\n",
        "    config_dict = {\n",
        "      \"additional_network_arguments\": {\n",
        "        \"unet_lr\": unet_lr,\n",
        "        \"text_encoder_lr\": text_encoder_lr,\n",
        "        \"network_dim\": network_dim,\n",
        "        \"network_alpha\": network_alpha,\n",
        "        \"network_module\": network_module,\n",
        "        \"network_args\": network_args,\n",
        "        \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "        \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "      },\n",
        "      \"optimizer_arguments\": {\n",
        "        \"learning_rate\": unet_lr,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "        \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "        \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "        \"optimizer_type\": optimizer,\n",
        "        \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "      },\n",
        "      \"training_arguments\": {\n",
        "        \"max_train_steps\": max_train_steps,\n",
        "        \"max_train_epochs\": max_train_epochs,\n",
        "        \"save_every_n_epochs\": save_every_n_epochs,\n",
        "        \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "        \"train_batch_size\": train_batch_size,\n",
        "        \"noise_offset\": None,\n",
        "        \"clip_skip\": 2,\n",
        "        \"min_snr_gamma\": min_snr_gamma_value,\n",
        "        \"weighted_captions\": weighted_captions,\n",
        "        \"seed\": 42,\n",
        "        \"max_token_length\": 225,\n",
        "        \"xformers\": XFORMERS,\n",
        "        \"lowram\": COLAB,\n",
        "        \"max_data_loader_n_workers\": 8,\n",
        "        \"persistent_data_loader_workers\": True,\n",
        "        \"save_precision\": \"fp16\",\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"output_dir\": output_folder,\n",
        "        \"logging_dir\": log_folder,\n",
        "        \"output_name\": project_name,\n",
        "        \"log_prefix\": project_name,\n",
        "      },\n",
        "      \"model_arguments\": {\n",
        "        \"pretrained_model_name_or_path\": model_file,\n",
        "        \"v2\": custom_model_is_based_on_sd2,\n",
        "        \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
        "      },\n",
        "      \"saving_arguments\": {\n",
        "        \"save_model_as\": \"safetensors\",\n",
        "      },\n",
        "      \"dreambooth_arguments\": {\n",
        "        \"prior_loss_weight\": 1.0,\n",
        "      },\n",
        "      \"dataset_arguments\": {\n",
        "        \"cache_latents\": True,\n",
        "      },\n",
        "    }\n",
        "\n",
        "    for key in config_dict:\n",
        "      if isinstance(config_dict[key], dict):\n",
        "        config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(config_dict))\n",
        "    print(f\"\\n📄 Config saved to {config_file}\")\n",
        "\n",
        "  if override_dataset_config_file:\n",
        "    dataset_config_file = override_dataset_config_file\n",
        "    print(f\"⭕ Using custom dataset config file {dataset_config_file}\")\n",
        "  else:\n",
        "    dataset_config_dict = {\n",
        "      \"general\": {\n",
        "        \"resolution\": resolution,\n",
        "        \"shuffle_caption\": shuffle_caption,\n",
        "        \"keep_tokens\": keep_tokens,\n",
        "        \"flip_aug\": flip_aug,\n",
        "        \"caption_extension\": caption_extension,\n",
        "        \"enable_bucket\": True,\n",
        "        \"bucket_reso_steps\": 64,\n",
        "        \"bucket_no_upscale\": False,\n",
        "        \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "        \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "      },\n",
        "      \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "        {\n",
        "          \"subsets\": [\n",
        "            {\n",
        "              \"num_repeats\": num_repeats,\n",
        "              \"image_dir\": images_folder,\n",
        "              \"class_tokens\": None if caption_extension else project_name\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "\n",
        "    for key in dataset_config_dict:\n",
        "      if isinstance(dataset_config_dict[key], dict):\n",
        "        dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "    with open(dataset_config_file, \"w\") as f:\n",
        "      f.write(toml.dumps(dataset_config_dict))\n",
        "    print(f\"📄 Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "    if os.path.exists(model_file):\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", model_url):\n",
        "    if m.group(2):\n",
        "      model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "    if m := re.search(r\"modelVersionId=([0-9]+)\", model_url):\n",
        "      real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "    else:\n",
        "      raise ValueError(\"optional_custom_training_model_url contains a civitai link, but the link doesn't include a modelVersionId. You can also right click the download button to copy the direct download link.\")\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except:\n",
        "      return False\n",
        "\n",
        "  return True"
      ],
      "metadata": {
        "id": "Hupa1xi7URbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\n🏭 Installing dependencies...\\n\")\n",
        "    t0 = time.time()\n",
        "    install_dependencies()\n",
        "    t1 = time.time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\n✅ Installation finished in {int(t1-t0)} seconds.\")\n",
        "  else:\n",
        "    print(\"\\n✅ Dependencies already installed.\")\n",
        "\n",
        "  global model_file\n",
        "  model_file = \"Mitsua/mitsua-diffusion-one\"\n",
        "  print(\"\\n🔄 Model already downloaded.\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n⭐ Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}"
      ],
      "metadata": {
        "id": "PoNyj_LBUXgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "import time"
      ],
      "metadata": {
        "id": "fl3BL10mUibP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in cats:\n",
        "  print(cat['name'])\n",
        "  if cat['name'] in ['handbag', 'sports ball', 'toaster', 'hair drier']:\n",
        "      continue\n",
        "  category = cat['name'].replace(\" \", \"_\")\n",
        "  if category in seen:\n",
        "    continue\n",
        "  project_name = category\n",
        "  project_name = project_name.strip()\n",
        "  main_dir = os.path.join(root_dir, \"gdrive/MyDrive/Loras\")\n",
        "  log_folder = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "  config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "  dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "  main()\n",
        "  time.sleep(120)"
      ],
      "metadata": {
        "id": "z484WPuKUizM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textual Inversion"
      ],
      "metadata": {
        "id": "AtWT6L-UVfF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install the required libs\n",
        "!pip install -U -qq git+https://github.com/huggingface/diffusers.git\n",
        "!pip install -qq accelerate transformers ftfy"
      ],
      "metadata": {
        "id": "eVVyOiFPVgQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import required libraries\n",
        "import argparse\n",
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import PIL\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
        "from diffusers.optimization import get_scheduler\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "import accelerate\n",
        "import time\n",
        "import shutil\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ],
      "metadata": {
        "id": "RMRI8A_sVqC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_name_or_path = \"Mitsua/mitsua-diffusion-one\"\n",
        "what_to_teach = \"object\""
      ],
      "metadata": {
        "id": "zj01iM_cVskC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup the prompt templates for training\n",
        "imagenet_templates_small = [\n",
        "    \"a photo of a {}\",\n",
        "    \"a rendering of a {}\",\n",
        "    \"a cropped photo of the {}\",\n",
        "    \"the photo of a {}\",\n",
        "    \"a photo of a clean {}\",\n",
        "    \"a photo of a dirty {}\",\n",
        "    \"a dark photo of the {}\",\n",
        "    \"a photo of my {}\",\n",
        "    \"a photo of the cool {}\",\n",
        "    \"a close-up photo of a {}\",\n",
        "    \"a bright photo of the {}\",\n",
        "    \"a cropped photo of a {}\",\n",
        "    \"a photo of the {}\",\n",
        "    \"a good photo of the {}\",\n",
        "    \"a photo of one {}\",\n",
        "    \"a close-up photo of the {}\",\n",
        "    \"a rendition of the {}\",\n",
        "    \"a photo of the clean {}\",\n",
        "    \"a rendition of a {}\",\n",
        "    \"a photo of a nice {}\",\n",
        "    \"a good photo of a {}\",\n",
        "    \"a photo of the nice {}\",\n",
        "    \"a photo of the small {}\",\n",
        "    \"a photo of the weird {}\",\n",
        "    \"a photo of the large {}\",\n",
        "    \"a photo of a cool {}\",\n",
        "    \"a photo of a small {}\",\n",
        "]\n",
        "\n",
        "imagenet_style_templates_small = [\n",
        "    \"a painting in the style of {}\",\n",
        "    \"a rendering in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"the painting in the style of {}\",\n",
        "    \"a clean painting in the style of {}\",\n",
        "    \"a dirty painting in the style of {}\",\n",
        "    \"a dark painting in the style of {}\",\n",
        "    \"a picture in the style of {}\",\n",
        "    \"a cool painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a bright painting in the style of {}\",\n",
        "    \"a cropped painting in the style of {}\",\n",
        "    \"a good painting in the style of {}\",\n",
        "    \"a close-up painting in the style of {}\",\n",
        "    \"a rendition in the style of {}\",\n",
        "    \"a nice painting in the style of {}\",\n",
        "    \"a small painting in the style of {}\",\n",
        "    \"a weird painting in the style of {}\",\n",
        "    \"a large painting in the style of {}\",\n",
        "]"
      ],
      "metadata": {
        "id": "nOB1UAS5Vt6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup the dataset\n",
        "class TextualInversionDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_root,\n",
        "        tokenizer,\n",
        "        learnable_property=\"object\",  # [object, style]\n",
        "        size=512,\n",
        "        repeats=100,\n",
        "        interpolation=\"bicubic\",\n",
        "        flip_p=0.5,\n",
        "        set=\"train\",\n",
        "        placeholder_token=\"*\",\n",
        "        center_crop=False,\n",
        "    ):\n",
        "\n",
        "        self.data_root = data_root\n",
        "        self.tokenizer = tokenizer\n",
        "        self.learnable_property = learnable_property\n",
        "        self.size = size\n",
        "        self.placeholder_token = placeholder_token\n",
        "        self.center_crop = center_crop\n",
        "        self.flip_p = flip_p\n",
        "\n",
        "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
        "\n",
        "        self.num_images = len(self.image_paths)\n",
        "        self._length = self.num_images\n",
        "\n",
        "        if set == \"train\":\n",
        "            self._length = self.num_images * repeats\n",
        "\n",
        "        self.interpolation = {\n",
        "            \"linear\": PIL.Image.BILINEAR,\n",
        "            \"bilinear\": PIL.Image.BILINEAR,\n",
        "            \"bicubic\": PIL.Image.BICUBIC,\n",
        "            \"lanczos\": PIL.Image.LANCZOS,\n",
        "        }[interpolation]\n",
        "\n",
        "        self.templates = imagenet_style_templates_small if learnable_property == \"style\" else imagenet_templates_small\n",
        "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        example = {}\n",
        "        image = Image.open(self.image_paths[i % self.num_images])\n",
        "\n",
        "        if not image.mode == \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        placeholder_string = self.placeholder_token\n",
        "        text = random.choice(self.templates).format(placeholder_string)\n",
        "\n",
        "        example[\"input_ids\"] = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).input_ids[0]\n",
        "\n",
        "        # default to score-sde preprocessing\n",
        "        img = np.array(image).astype(np.uint8)\n",
        "\n",
        "        if self.center_crop:\n",
        "            crop = min(img.shape[0], img.shape[1])\n",
        "            h, w, = (\n",
        "                img.shape[0],\n",
        "                img.shape[1],\n",
        "            )\n",
        "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
        "\n",
        "        image = Image.fromarray(img)\n",
        "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
        "\n",
        "        image = self.flip_transform(image)\n",
        "        image = np.array(image).astype(np.uint8)\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "\n",
        "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        return example"
      ],
      "metadata": {
        "id": "mJOfmetvVwVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_params(params):\n",
        "    for param in params:\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "id": "nwweX8HUVzpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_directory = TI_WORKSPACE"
      ],
      "metadata": {
        "id": "H2VRCUESV00b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training function\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "def save_progress(text_encoder, placeholder_token_id, accelerator, save_path):\n",
        "    logger.info(\"Saving embeddings\")\n",
        "    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n",
        "    learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n",
        "    torch.save(learned_embeds_dict, save_path)\n",
        "\n",
        "def training_function(text_encoder, vae, unet):\n",
        "    train_batch_size = hyperparameters[\"train_batch_size\"]\n",
        "    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n",
        "    learning_rate = hyperparameters[\"learning_rate\"]\n",
        "    max_train_steps = hyperparameters[\"max_train_steps\"]\n",
        "    output_dir = hyperparameters[\"output_dir\"]\n",
        "    gradient_checkpointing = hyperparameters[\"gradient_checkpointing\"]\n",
        "\n",
        "    accelerator = Accelerator(\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        mixed_precision=hyperparameters[\"mixed_precision\"]\n",
        "    )\n",
        "\n",
        "    if gradient_checkpointing:\n",
        "        text_encoder.gradient_checkpointing_enable()\n",
        "        unet.enable_gradient_checkpointing()\n",
        "\n",
        "    train_dataloader = create_dataloader(train_batch_size)\n",
        "\n",
        "    if hyperparameters[\"scale_lr\"]:\n",
        "        learning_rate = (\n",
        "            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n",
        "        )\n",
        "\n",
        "    # Initialize the optimizer\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        text_encoder.get_input_embeddings().parameters(),  # only optimize the embeddings\n",
        "        lr=learning_rate,\n",
        "    )\n",
        "\n",
        "    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
        "        text_encoder, optimizer, train_dataloader\n",
        "    )\n",
        "\n",
        "    weight_dtype = torch.float32\n",
        "    if accelerator.mixed_precision == \"fp16\":\n",
        "        weight_dtype = torch.float16\n",
        "    elif accelerator.mixed_precision == \"bf16\":\n",
        "        weight_dtype = torch.bfloat16\n",
        "\n",
        "    # Move vae and unet to device\n",
        "    vae.to(accelerator.device, dtype=weight_dtype)\n",
        "    unet.to(accelerator.device, dtype=weight_dtype)\n",
        "\n",
        "    # Keep vae in eval mode as we don't train it\n",
        "    vae.eval()\n",
        "    # Keep unet in train mode to enable gradient checkpointing\n",
        "    unet.train()\n",
        "\n",
        "\n",
        "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "    # Train!\n",
        "    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
        "\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
        "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
        "    # Only show the progress bar once on each machine.\n",
        "    progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "    progress_bar.set_description(\"Steps\")\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(num_train_epochs):\n",
        "        text_encoder.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(text_encoder):\n",
        "                # Convert images to latent space\n",
        "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample().detach()\n",
        "                latents = latents * 0.18215\n",
        "\n",
        "                # Sample noise that we'll add to the latents\n",
        "                noise = torch.randn_like(latents)\n",
        "                bsz = latents.shape[0]\n",
        "                # Sample a random timestep for each image\n",
        "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n",
        "\n",
        "                # Add noise to the latents according to the noise magnitude at each timestep\n",
        "                # (this is the forward diffusion process)\n",
        "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "                # Get the text embedding for conditioning\n",
        "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "                # Predict the noise residual\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states.to(weight_dtype)).sample\n",
        "\n",
        "                 # Get the target for loss depending on the prediction type\n",
        "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                    target = noise\n",
        "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "                loss = F.mse_loss(noise_pred, target, reduction=\"none\").mean([1, 2, 3]).mean()\n",
        "                accelerator.backward(loss)\n",
        "\n",
        "                # Zero out the gradients for all token embeddings except the newly added\n",
        "                # embeddings for the concept, as we only want to optimize the concept embeddings\n",
        "                if accelerator.num_processes > 1:\n",
        "                    grads = text_encoder.module.get_input_embeddings().weight.grad\n",
        "                else:\n",
        "                    grads = text_encoder.get_input_embeddings().weight.grad\n",
        "                # Get the index for tokens that we want to zero the grads for\n",
        "                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
        "                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "                if global_step % hyperparameters[\"save_steps\"] == 0:\n",
        "                    save_path = os.path.join(output_dir, f\"learned_embeds-step-{global_step}.bin\")\n",
        "                    save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n",
        "\n",
        "            logs = {\"loss\": loss.detach().item()}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "\n",
        "            if global_step >= max_train_steps:\n",
        "                break\n",
        "\n",
        "        accelerator.wait_for_everyone()\n",
        "\n",
        "\n",
        "    # Create the pipeline using using the trained modules and save it.\n",
        "    if accelerator.is_main_process:\n",
        "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "            pretrained_model_name_or_path,\n",
        "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
        "            tokenizer=tokenizer,\n",
        "            vae=vae,\n",
        "            unet=unet,\n",
        "            safety_checker=None\n",
        "        )\n",
        "        pipeline.save_pretrained(output_dir)\n",
        "        # Also save the newly trained embeddings\n",
        "        save_path = os.path.join(output_dir, f\"learned_embeds.bin\")\n",
        "        save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n",
        "        del pipeline"
      ],
      "metadata": {
        "id": "o6zT_XWBV8Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for cat in cats:\n",
        "    print(cat['name'])\n",
        "    if cat['name'] in seen:\n",
        "        continue\n",
        "    if cat['name'] in ['handbag', 'sports ball', 'toaster', 'hair drier']:\n",
        "        continue\n",
        "    category = cat['name'].replace(' ', '_')\n",
        "\n",
        "    images_path = \"/content/gdrive/MyDrive/Fast-Dreambooth/Sessions/\" + category + \"/instance_images/\"\n",
        "    save_path = images_path\n",
        "    placeholder_token = \"<\" + cat['name'] + \">\"\n",
        "    initializer_token = cat['name']\n",
        "\n",
        "    # Load the tokenizer and add the placeholder token as a additional special token.\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(\n",
        "        pretrained_model_name_or_path,\n",
        "        subfolder=\"tokenizer\",\n",
        "    )\n",
        "\n",
        "    # Add the placeholder token in tokenizer\n",
        "    num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
        "    if num_added_tokens == 0:\n",
        "        raise ValueError(\n",
        "            f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n",
        "            \" `placeholder_token` that is not already in the tokenizer.\"\n",
        "        )\n",
        "\n",
        "    # Get token ids for our placeholder and initializer token. This code block will complain if initializer string is not a single token\n",
        "    # Convert the initializer_token, placeholder_token to ids\n",
        "    token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n",
        "\n",
        "    initializer_token_id = token_ids[0]\n",
        "    placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
        "\n",
        "    # Load the Stable Diffusion model\n",
        "    # Load models and create wrapper for stable diffusion\n",
        "    #pipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path)\n",
        "    text_encoder = CLIPTextModel.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
        "    )\n",
        "    vae = AutoencoderKL.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"vae\"\n",
        "    )\n",
        "    unet = UNet2DConditionModel.from_pretrained(\n",
        "        pretrained_model_name_or_path, subfolder=\"unet\"\n",
        "    )\n",
        "\n",
        "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
        "    token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]\n",
        "\n",
        "    # Freeze vae and unet\n",
        "    freeze_params(vae.parameters())\n",
        "    freeze_params(unet.parameters())\n",
        "    # Freeze all parameters except for the token embeddings in text encoder\n",
        "    params_to_freeze = itertools.chain(\n",
        "        text_encoder.text_model.encoder.parameters(),\n",
        "        text_encoder.text_model.final_layer_norm.parameters(),\n",
        "        text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
        "    )\n",
        "    freeze_params(params_to_freeze)\n",
        "\n",
        "    train_dataset = TextualInversionDataset(\n",
        "      data_root=save_path,\n",
        "      tokenizer=tokenizer,\n",
        "      size=vae.sample_size,\n",
        "      placeholder_token=placeholder_token,\n",
        "      repeats=100,\n",
        "      learnable_property=what_to_teach, #Option selected above between object and style\n",
        "      center_crop=False,\n",
        "      set=\"train\",\n",
        "    )\n",
        "\n",
        "    noise_scheduler = DDPMScheduler.from_config(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "    #@title Setting up all training args\n",
        "    hyperparameters = {\n",
        "        \"learning_rate\": 5e-04,\n",
        "        \"scale_lr\": True,\n",
        "        \"max_train_steps\": 2000,\n",
        "        \"save_steps\": 250,\n",
        "        \"train_batch_size\": 4,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"gradient_checkpointing\": True,\n",
        "        \"mixed_precision\": \"fp16\",\n",
        "        \"seed\": 42,\n",
        "        \"output_dir\": \"sd-concept-output\"\n",
        "    }\n",
        "    !mkdir -p sd-concept-output\n",
        "\n",
        "    accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet))\n",
        "\n",
        "    for param in itertools.chain(unet.parameters(), text_encoder.parameters()):\n",
        "      if param.grad is not None:\n",
        "        del param.grad  # free some memory\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    # Source and destination paths\n",
        "    source = \"/content/sd-concept-output/learned_embeds.bin\"\n",
        "    destination = TI_WORKSPACE + cat['name']\n",
        "    os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "    # Move the file\n",
        "    shutil.move(source, destination)\n",
        "    print(f\"File moved from {source} to {destination}\")\n",
        "\n",
        "    time.sleep(120)\n",
        "\n",
        "    # Replace 'folder_path' with the path to the folder you want to delete\n",
        "    folder_path = '/content/sd-concept-output'\n",
        "    # Force delete the folder\n",
        "    shutil.rmtree(folder_path, ignore_errors=True)\n",
        "    print(f\"Folder {folder_path} has been deleted.\")"
      ],
      "metadata": {
        "id": "q1kUet-UV-RZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}